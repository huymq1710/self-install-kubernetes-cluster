tags: worker, kube-proxy

# 06-5. Deploy kube-proxy component

<!-- TOC -->

- [06-5. Deploy kube-proxy component] (#06-5-Deployment-kube-proxy-component)
     - [Download and distribute kube-proxy binary](#download and distribute-kube-proxy-binary)
     - [Create kube-proxy certificate](#create-kube-proxy-certificate)
     - [Create and distribute kubeconfig files](#create and distribute-kubeconfig-files)
     - [Create kube-proxy configuration file](#create-kube-proxy-configuration file)
     - [Create and distribute kube-proxy systemd unit file](#create and distribute-kube-proxy-systemd-unit-file)
     - [Start kube-proxy service](#start-kube-proxy-service)
     - [Check startup results](#check startup results)
     - [View listening port](#View listening port)
     - [View ipvs routing rules](#view-ipvs-routing rules)

<!-- /TOC -->

kube-proxy runs on all worker nodes. It monitors changes in services and endpoints in apiserver, and creates routing rules to provide service IP and load balancing functions.

This document explains the process of deploying kube-proxy in ipvs mode.

Note: Unless otherwise specified, all operations in this document are performed on the idc-k8s-01 node, and then files and commands are distributed remotely.

## Download and distribute the kube-proxy binaries

Refer to [05-1.Deploy master node.md](05-1.Deploy master node.md).

## Create kube-proxy certificate

Create a certificate signing request:

``` bash
cd /opt/k8s/work
cat > kube-proxy-csr.json <<EOF
{
  "CN": "system:kube-proxy",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "BeiJing",
      "L": "BeiJing",
      "O": "k8s",
      "OU": "opsnull"
    }
  ]
}
EOF
```
+ CN: Specify the User of this certificate as `system:kube-proxy`;
+ The predefined RoleBinding `system:node-proxier` binds the User `system:kube-proxy` to the Role `system:node-proxier`, which grants the permission to call the `kube-apiserver` Proxy related API;
+ This certificate will only be used by kube-proxy as a client certificate, so the hosts field is empty;

Generate certificate and private key:

``` bash
cd /opt/k8s/work
cfssl gencert -ca=/opt/k8s/work/ca.pem \
  -ca-key=/opt/k8s/work/ca-key.pem \
  -config=/opt/k8s/work/ca-config.json \
  -profile=kubernetes  kube-proxy-csr.json | cfssljson -bare kube-proxy
ls kube-proxy*
```
```
kube-proxy-csr.json  kube-proxy-key.pem  kube-proxy.csr  kube-proxy.pem
```

## Create and distribute kubeconfig files

``` bash
cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
kubectl config set-cluster kubernetes \
  --certificate-authority=/opt/k8s/work/ca.pem \
  --embed-certs=true \
  --server=${KUBE_APISERVER} \
  --kubeconfig=kube-proxy.kubeconfig

kubectl config set-credentials kube-proxy \
  --client-certificate=kube-proxy.pem \
  --client-key=kube-proxy-key.pem \
  --embed-certs=true \
  --kubeconfig=kube-proxy.kubeconfig

kubectl config set-context default \
  --cluster=kubernetes \
  --user=kube-proxy \
  --kubeconfig=kube-proxy.kubeconfig

kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig
```

Distribute the kubeconfig file:

``` bash
cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
for node_name in ${NODE_NAMES[@]}
  do
    echo ">>> ${node_name}"
    scp kube-proxy.kubeconfig root@${node_name}:/etc/kubernetes/
  done
```
```
>>> idc-k8s-01
kube-proxy.kubeconfig                                                                                    100% 6229     4.0MB/s   00:00
>>> idc-k8s-02
kube-proxy.kubeconfig                                                                                    100% 6229     5.3MB/s   00:00
>>> idc-k8s-03
kube-proxy.kubeconfig                                                                                    100% 6229     3.4MB/s   00:00
```
## Create kube-proxy configuration file

Starting from v1.10, kube-proxy **some parameters** can be configured in the configuration file. You can use the `--write-config-to` option to generate this configuration file, or refer to [Source code comments](https://github.com/kubernetes/kubernetes/blob/release-1.14/pkg/proxy/apis/config/types.go).

Create kube-proxy config file template:

``` bash
cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
cat > kube-proxy-config.yaml.template <<EOF
kind: KubeProxyConfiguration
apiVersion: kubeproxy.config.k8s.io/v1alpha1
clientConnection:
  burst: 200
  kubeconfig: "/etc/kubernetes/kube-proxy.kubeconfig"
  qps: 100
bindAddress: ##NODE_IP##
healthzBindAddress: ##NODE_IP##:10256
metricsBindAddress: ##NODE_IP##:10249
enableProfiling: true
clusterCIDR: ${CLUSTER_CIDR}
hostnameOverride: ##NODE_NAME##
mode: "ipvs"
portRange: ""
iptables:
  masqueradeAll: false
ipvs:
  scheduler: rr
  excludeCIDRs: []
EOF
```
+ `bindAddress`: listening address;
+ `clientConnection.kubeconfig`: kubeconfig file to connect to apiserver;
+ `clusterCIDR`: kube-proxy determines the internal and external traffic of the cluster based on `--cluster-cidr`. Only after specifying the `--cluster-cidr` or `--masquerade-all` option will kube-proxy access the Service IP The request does SNAT;
+ `hostnameOverride`: The parameter value must be consistent with the value of kubelet, otherwise kube-proxy will not be able to find the Node after starting, and thus no ipvs rules will be created;
+ `mode`: use ipvs mode;

Create and distribute kube-proxy configuration files to each node:

``` bash
cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
for (( i=0; i < 3; i++ ))
  do 
    echo ">>> ${NODE_NAMES[i]}"
    sed -e "s/##NODE_NAME##/${NODE_NAMES[i]}/" -e "s/##NODE_IP##/${NODE_IPS[i]}/" kube-proxy-config.yaml.template > kube-proxy-config-${NODE_NAMES[i]}.yaml.template
    scp kube-proxy-config-${NODE_NAMES[i]}.yaml.template root@${NODE_NAMES[i]}:/etc/kubernetes/kube-proxy-config.yaml
  done
```
```
>>> idc-k8s-01
kube-proxy-config-idc-k8s-01.yaml.template                                                               100%  456   533.6KB/s   00:00
>>> idc-k8s-02
kube-proxy-config-idc-k8s-02.yaml.template                                                               100%  459   494.0KB/s   00:00
>>> idc-k8s-03
kube-proxy-config-idc-k8s-03.yaml.template                                                               100%  459   256.7KB/s   00:00
```
## Create and distribute kube-proxy systemd unit files

``` bash
cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
cat > kube-proxy.service <<EOF
[Unit]
Description=Kubernetes Kube-Proxy Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target

[Service]
WorkingDirectory=${K8S_DIR}/kube-proxy
ExecStart=/opt/k8s/bin/kube-proxy \\
  --config=/etc/kubernetes/kube-proxy-config.yaml \\
  --logtostderr=true \\
  --v=2
Restart=on-failure
RestartSec=5
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF
```

Distribute the kube-proxy systemd unit files:

``` bash
cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
for node_name in ${NODE_NAMES[@]}
  do 
    echo ">>> ${node_name}"
    scp kube-proxy.service root@${node_name}:/etc/systemd/system/
  done
```
```
>>> idc-k8s-01
kube-proxy.service                                                                                       100%  393    21.3KB/s   00:00
>>> idc-k8s-02
kube-proxy.service                                                                                       100%  393   207.8KB/s   00:00
>>> idc-k8s-03
kube-proxy.service                                                                                       100%  393   166.7KB/s   00:00
```
## Start the kube-proxy service

``` bash
cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo ">>> ${node_ip}"
    ssh root@${node_ip} "mkdir -p ${K8S_DIR}/kube-proxy"
    ssh root@${node_ip} "modprobe ip_vs_rr"
    ssh root@${node_ip} "systemctl daemon-reload && systemctl enable kube-proxy && systemctl restart kube-proxy"
  done
```

## Check startup results

``` bash
source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo ">>> ${node_ip}"
    ssh root@${node_ip} "systemctl status kube-proxy|grep Active"
  done
```

Make sure the status is `active (running)`, otherwise check the log to confirm the reason:

``` bash
journalctl -u kube-proxy
-- Logs begin at Sun 2024-03-31 12:02:38 +07, end at Mon 2024-04-01 15:02:40 +07. --
Apr 01 15:02:30 idc-k8s-01 systemd[1]: Started Kubernetes Kube-Proxy Server.
Apr 01 15:02:30 idc-k8s-01 kube-proxy[23224]: I0401 15:02:30.167570   23224 flags.go:33] FLAG: --add-dir-header="false"
Apr 01 15:02:30 idc-k8s-01 kube-proxy[23224]: I0401 15:02:30.167660   23224 flags.go:33] FLAG: --alsologtostderr="false"
Apr 01 15:02:30 idc-k8s-01 kube-proxy[23224]: I0401 15:02:30.167676   23224 flags.go:33] FLAG: --bind-address="0.0.0.0"
Apr 01 15:02:30 idc-k8s-01 kube-proxy[23224]: I0401 15:02:30.167693   23224 flags.go:33] FLAG: --cleanup="false"
Apr 01 15:02:30 idc-k8s-01 kube-proxy[23224]: I0401 15:02:30.167708   23224 flags.go:33] FLAG: --cleanup-ipvs="true"
Apr 01 15:02:30 idc-k8s-01 kube-proxy[23224]: I0401 15:02:30.167723   23224 flags.go:33] FLAG: --cluster-cidr=""
Apr 01 15:02:30 idc-k8s-01 kube-proxy[23224]: I0401 15:02:30.167741   23224 flags.go:33] FLAG: --config="/etc/kubernetes/kube-proxy-config.
Apr 01 15:02:30 idc-k8s-01 kube-proxy[23224]: I0401 15:02:30.167753   23224 flags.go:33] FLAG: --config-sync-period="15m0s"
Apr 01 15:02:30 idc-k8s-01 kube-proxy[23224]: I0401 15:02:30.167763   23224 flags.go:33] FLAG: --conntrack-max-per-core="32768"
Apr 01 15:02:30 idc-k8s-01 kube-proxy[23224]: I0401 15:02:30.167783   23224 flags.go:33] FLAG: --conntrack-min="131072"
Apr 01 15:02:30 idc-k8s-01 kube-proxy[23224]: I0401 15:02:30.167792   23224 flags.go:33] FLAG: --conntrack-tcp-timeout-close-wait="1h0m0s"
Apr 01 15:02:30 idc-k8s-01 kube-proxy[23224]: I0401 15:02:30.167806   23224 flags.go:33] FLAG: --conntrack-tcp-timeout-established="24h0m0s
Apr 01 15:02:30 idc-k8s-01 kube-proxy[23224]: I0401 15:02:30.167815   23224 flags.go:33] FLAG: --feature-gates=""
Apr 01 15:02:30 idc-k8s-01 kube-proxy[23224]: I0401 15:02:30.167825   23224 flags.go:33] FLAG: --healthz-bind-address="0.0.0.0:10256"
Apr 01 15:02:30 idc-k8s-01 kube-proxy[23224]: I0401 15:02:30.167833   23224 flags.go:33] FLAG: --healthz-port="10256"
Apr 01 15:02:30 idc-k8s-01 kube-proxy[23224]: I0401 15:02:30.167842   23224 flags.go:33] FLAG: --help="false"
Apr 01 15:02:30 idc-k8s-01 kube-proxy[23224]: I0401 15:02:30.167850   23224 flags.go:33] FLAG: --hostname-override=""
Apr 01 15:02:30 idc-k8s-01 kube-proxy[23224]: I0401 15:02:30.167858   23224 flags.go:33] FLAG: --iptables-masquerade-bit="14"
Apr 01 15:02:30 idc-k8s-01 kube-proxy[23224]: I0401 15:02:30.167866   23224 flags.go:33] FLAG: --iptables-min-sync-period="0s"
Apr 01 15:02:30 idc-k8s-01 kube-proxy[23224]: I0401 15:02:30.167874   23224 flags.go:33] FLAG: --iptables-sync-period="30s"
Apr 01 15:02:30 idc-k8s-01 kube-proxy[23224]: I0401 15:02:30.167881   23224 flags.go:33] FLAG: --ipvs-exclude-cidrs="[]"
Apr 01 15:02:30 idc-k8s-01 kube-proxy[23224]: I0401 15:02:30.167897   23224 flags.go:33] FLAG: --ipvs-min-sync-period="0s"
Apr 01 15:02:30 idc-k8s-01 kube-proxy[23224]: I0401 15:02:30.167906   23224 flags.go:33] FLAG: --ipvs-scheduler=""
Apr 01 15:02:30 idc-k8s-01 kube-proxy[23224]: I0401 15:02:30.167913   23224 flags.go:33] FLAG: --ipvs-strict-arp="false"
Apr 01 15:02:30 idc-k8s-01 kube-proxy[23224]: I0401 15:02:30.167929   23224 flags.go:33] FLAG: --ipvs-sync-period="30s"
Apr 01 15:02:30 idc-k8s-01 kube-proxy[23224]: I0401 15:02:30.167944   23224 flags.go:33] FLAG: --kube-api-burst="10"
Apr 01 15:02:30 idc-k8s-01 kube-proxy[23224]: I0401 15:02:30.167953   23224 flags.go:33] FLAG: --kube-api-content-type="application/vnd.kub
Apr 01 15:02:30 idc-k8s-01 kube-proxy[23224]: I0401 15:02:30.167961   23224 flags.go:33] FLAG: --kube-api-qps="5"
Apr 01 15:02:30 idc-k8s-01 kube-proxy[23224]: I0401 15:02:30.167972   23224 flags.go:33] FLAG: --kubeconfig=""
Apr 01 15:02:30 idc-k8s-01 kube-proxy[23224]: I0401 15:02:30.167980   23224 flags.go:33] FLAG: --log-backtrace-at=":0"
Apr 01 15:02:30 idc-k8s-01 kube-proxy[23224]: I0401 15:02:30.167992   23224 flags.go:33] FLAG: --log-dir=""
Apr 01 15:02:30 idc-k8s-01 kube-proxy[23224]: I0401 15:02:30.168000   23224 flags.go:33] FLAG: --log-file=""
```

## Check the listening port

``` bash
$ sudo netstat -lnpt|grep kube-prox
tcp        0      0 103.172.239.9:10249     0.0.0.0:*               LISTEN      23224/kube-proxy
tcp        0      0 103.172.239.9:10256     0.0.0.0:*               LISTEN      23224/kube-proxy
```
+ 10249：http prometheus metrics port;
+ 10256：http healthz port;

## View ipvs routing rules

``` bash
source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo ">>> ${node_ip}"
    ssh root@${node_ip} "/usr/sbin/ipvsadm -ln"
  done
```

Expected output:

``` bash
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -> RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  10.96.0.1:443 rr
  -> 103.172.238.94:6443          Masq    1      0          0
  -> 103.172.239.9:6443           Masq    1      0          0
  -> 103.172.239.71:6443          Masq    1      0          0
>>> 103.172.238.94
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -> RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  10.96.0.1:443 rr
  -> 103.172.238.94:6443          Masq    1      0          0
  -> 103.172.239.9:6443           Masq    1      0          0
  -> 103.172.239.71:6443          Masq    1      0          0
>>> 103.172.239.71
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -> RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  10.96.0.1:443 rr
  -> 103.172.238.94:6443          Masq    1      0          0
  -> 103.172.239.9:6443           Masq    1      0          0
  -> 103.172.239.71:6443          Masq    1      0          0
```

It can be seen that all requests to access K8S SVC kubernetes through https are forwarded to port 6443 of the kube-apiserver node;