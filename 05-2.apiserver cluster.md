tags: master, kube-apiserver

# 05-2. Deploy kube-apiserver cluster

<!-- TOC -->

- [05-2. Deploy kube-apiserver cluster](#05-2-deployment-kube-apiserver-cluster)
     - [Create kubernetes-master certificate and private key](#create-kubernetes-master-certificate and private key)
     - [Create encryption configuration file](#Create encryption configuration file)
     - [Create audit policy file](#Create audit policy file)
     - Create a certificate for subsequent access to metrics-server or kube-prometheus
     - [Create kube-apiserver systemd unit template file](#create-kube-apiserver-systemd-unit-template file)
     - [Create and distribute kube-apiserver systemd unit files for each node](#Create and distribute-kube-apiserver-systemd-unit-file for each node)
     - [Start kube-apiserver service](#start-kube-apiserver-service)
     - [Check kube-apiserver running status](#check-kube-apiserver-running status)
     - [Check cluster information](#check cluster information)
     - [Check kube-apiserver listening port](#check-kube-apiserver-listening port)

<!-- /TOC -->

This document explains the steps to deploy a three-instance kube-apiserver cluster.

Note: Unless otherwise specified, all operations in this document are performed on the idc-k8s-01 node.

## Create kubernetes-master certificate and private key

Create a certificate signing request:

``` bash
cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
cat > kubernetes-csr.json <<EOF
{
  "CN": "kubernetes-master",
  "hosts": [
    "127.0.0.1",
    "103.172.239.9",  // idc-k8s-01
    "103.172.238.94", // idc-k8s-02
    "103.172.239.71", // idc-k8s-03
    "${CLUSTER_KUBERNETES_SVC_IP}",
    "kubernetes",
    "kubernetes.default",
    "kubernetes.default.svc",
    "kubernetes.default.svc.cluster",
    "kubernetes.default.svc.cluster.local.",
    "kubernetes.default.svc.${CLUSTER_DNS_DOMAIN}."
  ],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "BeiJing",
      "L": "BeiJing",
      "O": "k8s",
      "OU": "opsnull"
    }
  ]
}
EOF
```
+ The hosts field specifies the **IP and domain name list** authorized to use the certificate. Here, the master node IP, the IP and domain name of the kubernetes service are listed;

Generate certificate and private key:

``` bash
cfssl gencert -ca=/opt/k8s/work/ca.pem \
  -ca-key=/opt/k8s/work/ca-key.pem \
  -config=/opt/k8s/work/ca-config.json \
  -profile=kubernetes kubernetes-csr.json | cfssljson -bare kubernetes
ls kubernetes*pem
```

```
2024/04/01 11:38:59 [INFO] generate received request
2024/04/01 11:38:59 [INFO] received CSR
2024/04/01 11:38:59 [INFO] generating key: rsa-2048
2024/04/01 11:39:00 [INFO] encoded CSR
2024/04/01 11:39:00 [INFO] signed certificate with serial number 698402941443197593865259030067506725516157390046

kubernetes-key.pem  kubernetes.pem
```

Copy the generated certificate and private key files to all master nodes:

``` bash
cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo ">>> ${node_ip}"
    ssh root@${node_ip} "mkdir -p /etc/kubernetes/cert"
    scp kubernetes*.pem root@${node_ip}:/etc/kubernetes/cert/
  done
```

## Create an encryption profile

``` bash
cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
cat > encryption-config.yaml <<EOF
kind: EncryptionConfig
apiVersion: v1
resources:
  - resources:
      - secrets
    providers:
      - aescbc:
          keys:
            - name: key1
              secret: ${ENCRYPTION_KEY}
      - identity: {}
EOF
```

```yaml
[root@idc-k8s-01 work]# cat encryption-config.yaml
kind: EncryptionConfig
apiVersion: v1
resources:
  - resources:
      - secrets
    providers:
      - aescbc:
          keys:
            - name: key1
              secret: f8qDQJ/v0J/Ol20goB0Xxed4NP+j+cIQeCD5e1t5MH0=
      - identity: {}
```

Copy the encryption configuration file to the `/etc/kubernetes` directory of the master node:

``` bash
cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo ">>> ${node_ip}"
    scp encryption-config.yaml root@${node_ip}:/etc/kubernetes/
  done
```

## Create audit policy file

``` bash
cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
cat > audit-policy.yaml <<EOF
apiVersion: audit.k8s.io/v1beta1
kind: Policy
rules:
  # The following requests were manually identified as high-volume and low-risk, so drop them.
  - level: None
    resources:
      - group: ""
        resources:
          - endpoints
          - services
          - services/status
    users:
      - 'system:kube-proxy'
    verbs:
      - watch

  - level: None
    resources:
      - group: ""
        resources:
          - nodes
          - nodes/status
    userGroups:
      - 'system:nodes'
    verbs:
      - get

  - level: None
    namespaces:
      - kube-system
    resources:
      - group: ""
        resources:
          - endpoints
    users:
      - 'system:kube-controller-manager'
      - 'system:kube-scheduler'
      - 'system:serviceaccount:kube-system:endpoint-controller'
    verbs:
      - get
      - update

  - level: None
    resources:
      - group: ""
        resources:
          - namespaces
          - namespaces/status
          - namespaces/finalize
    users:
      - 'system:apiserver'
    verbs:
      - get

  # Don't log HPA fetching metrics.
  - level: None
    resources:
      - group: metrics.k8s.io
    users:
      - 'system:kube-controller-manager'
    verbs:
      - get
      - list

  # Don't log these read-only URLs.
  - level: None
    nonResourceURLs:
      - '/healthz*'
      - /version
      - '/swagger*'

  # Don't log events requests.
  - level: None
    resources:
      - group: ""
        resources:
          - events

  # node and pod status calls from nodes are high-volume and can be large, don't log responses
  # for expected updates from nodes
  - level: Request
    omitStages:
      - RequestReceived
    resources:
      - group: ""
        resources:
          - nodes/status
          - pods/status
    users:
      - kubelet
      - 'system:node-problem-detector'
      - 'system:serviceaccount:kube-system:node-problem-detector'
    verbs:
      - update
      - patch

  - level: Request
    omitStages:
      - RequestReceived
    resources:
      - group: ""
        resources:
          - nodes/status
          - pods/status
    userGroups:
      - 'system:nodes'
    verbs:
      - update
      - patch

  # deletecollection calls can be large, don't log responses for expected namespace deletions
  - level: Request
    omitStages:
      - RequestReceived
    users:
      - 'system:serviceaccount:kube-system:namespace-controller'
    verbs:
      - deletecollection

  # Secrets, ConfigMaps, and TokenReviews can contain sensitive & binary data,
  # so only log at the Metadata level.
  - level: Metadata
    omitStages:
      - RequestReceived
    resources:
      - group: ""
        resources:
          - secrets
          - configmaps
      - group: authentication.k8s.io
        resources:
          - tokenreviews
  # Get repsonses can be large; skip them.
  - level: Request
    omitStages:
      - RequestReceived
    resources:
      - group: ""
      - group: admissionregistration.k8s.io
      - group: apiextensions.k8s.io
      - group: apiregistration.k8s.io
      - group: apps
      - group: authentication.k8s.io
      - group: authorization.k8s.io
      - group: autoscaling
      - group: batch
      - group: certificates.k8s.io
      - group: extensions
      - group: metrics.k8s.io
      - group: networking.k8s.io
      - group: policy
      - group: rbac.authorization.k8s.io
      - group: scheduling.k8s.io
      - group: settings.k8s.io
      - group: storage.k8s.io
    verbs:
      - get
      - list
      - watch

  # Default level for known APIs
  - level: RequestResponse
    omitStages:
      - RequestReceived
    resources:
      - group: ""
      - group: admissionregistration.k8s.io
      - group: apiextensions.k8s.io
      - group: apiregistration.k8s.io
      - group: apps
      - group: authentication.k8s.io
      - group: authorization.k8s.io
      - group: autoscaling
      - group: batch
      - group: certificates.k8s.io
      - group: extensions
      - group: metrics.k8s.io
      - group: networking.k8s.io
      - group: policy
      - group: rbac.authorization.k8s.io
      - group: scheduling.k8s.io
      - group: settings.k8s.io
      - group: storage.k8s.io
      
  # Default level for all other requests.
  - level: Metadata
    omitStages:
      - RequestReceived
EOF
```

Distribute the audit policy file:

``` bash
cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo ">>> ${node_ip}"
    scp audit-policy.yaml root@${node_ip}:/etc/kubernetes/audit-policy.yaml
  done
```

## Create a certificate for subsequent access to metrics-server or kube-prometheus

Create a certificate signing request:

``` bash
cd /opt/k8s/work
cat > proxy-client-csr.json <<EOF
{
  "CN": "aggregator",
  "hosts": [],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "BeiJing",
      "L": "BeiJing",
      "O": "k8s",
      "OU": "opsnull"
    }
  ]
}
EOF
```
+ The CN name needs to be in the `--requestheader-allowed-names` parameter of kube-apiserver, otherwise you will be prompted for insufficient permissions when accessing metrics later.

Generate certificate and private key:

``` bash
cfssl gencert -ca=/etc/kubernetes/cert/ca.pem \
  -ca-key=/etc/kubernetes/cert/ca-key.pem  \
  -config=/etc/kubernetes/cert/ca-config.json  \
  -profile=kubernetes proxy-client-csr.json | cfssljson -bare proxy-client
ls proxy-client*.pem
```
```
2024/04/01 11:43:00 [INFO] generate received request
2024/04/01 11:43:00 [INFO] received CSR
2024/04/01 11:43:00 [INFO] generating key: rsa-2048
2024/04/01 11:43:00 [INFO] encoded CSR
2024/04/01 11:43:00 [INFO] signed certificate with serial number 516332319293019555903855623894546194825328658253
2024/04/01 11:43:00 [WARNING] This certificate lacks a "hosts" field. This makes it unsuitable for
websites. For more information see the Baseline Requirements for the Issuance and Management
of Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);
specifically, section 10.2.3 ("Information Requirements").

[root@idc-k8s-01 work]# ls proxy-client*.pem
proxy-client-key.pem  proxy-client.pem
```

Copy the generated certificate and private key files to all master nodes:

``` bash
source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo ">>> ${node_ip}"
    scp proxy-client*.pem root@${node_ip}:/etc/kubernetes/cert/
  done
```

## Create kube-apiserver systemd unit template file

``` bash
cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
cat > kube-apiserver.service.template <<EOF
[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target

[Service]
WorkingDirectory=${K8S_DIR}/kube-apiserver
ExecStart=/opt/k8s/bin/kube-apiserver \\
  --advertise-address=##NODE_IP## \\
  --default-not-ready-toleration-seconds=360 \\
  --default-unreachable-toleration-seconds=360 \\
  --feature-gates=DynamicAuditing=true \\
  --max-mutating-requests-inflight=2000 \\
  --max-requests-inflight=4000 \\
  --default-watch-cache-size=200 \\
  --delete-collection-workers=2 \\
  --encryption-provider-config=/etc/kubernetes/encryption-config.yaml \\
  --etcd-cafile=/etc/kubernetes/cert/ca.pem \\
  --etcd-certfile=/etc/kubernetes/cert/kubernetes.pem \\
  --etcd-keyfile=/etc/kubernetes/cert/kubernetes-key.pem \\
  --etcd-servers=${ETCD_ENDPOINTS} \\
  --bind-address=##NODE_IP## \\
  --secure-port=6443 \\
  --tls-cert-file=/etc/kubernetes/cert/kubernetes.pem \\
  --tls-private-key-file=/etc/kubernetes/cert/kubernetes-key.pem \\
  --insecure-port=0 \\
  --audit-dynamic-configuration \\
  --audit-log-maxage=15 \\
  --audit-log-maxbackup=3 \\
  --audit-log-maxsize=100 \\
  --audit-log-truncate-enabled \\
  --audit-log-path=${K8S_DIR}/kube-apiserver/audit.log \\
  --audit-policy-file=/etc/kubernetes/audit-policy.yaml \\
  --profiling \\
  --anonymous-auth=false \\
  --client-ca-file=/etc/kubernetes/cert/ca.pem \\
  --enable-bootstrap-token-auth \\
  --requestheader-allowed-names="aggregator" \\
  --requestheader-client-ca-file=/etc/kubernetes/cert/ca.pem \\
  --requestheader-extra-headers-prefix="X-Remote-Extra-" \\
  --requestheader-group-headers=X-Remote-Group \\
  --requestheader-username-headers=X-Remote-User \\
  --service-account-key-file=/etc/kubernetes/cert/ca.pem \\
  --authorization-mode=Node,RBAC \\
  --runtime-config=api/all=true \\
  --enable-admission-plugins=NodeRestriction \\
  --allow-privileged=true \\
  --apiserver-count=3 \\
  --event-ttl=168h \\
  --kubelet-certificate-authority=/etc/kubernetes/cert/ca.pem \\
  --kubelet-client-certificate=/etc/kubernetes/cert/kubernetes.pem \\
  --kubelet-client-key=/etc/kubernetes/cert/kubernetes-key.pem \\
  --kubelet-https=true \\
  --kubelet-timeout=10s \\
  --proxy-client-cert-file=/etc/kubernetes/cert/proxy-client.pem \\
  --proxy-client-key-file=/etc/kubernetes/cert/proxy-client-key.pem \\
  --service-cluster-ip-range=${SERVICE_CIDR} \\
  --service-node-port-range=${NODE_PORT_RANGE} \\
  --logtostderr=true \\
  --v=2
Restart=on-failure
RestartSec=10
Type=notify
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF
```

+ `--advertise-address`: IP advertised by apiserver (kubernetes service backend node IP);
+ `--default-*-toleration-seconds`: Set the threshold related to node abnormality;
+ `--max-*-requests-inflight`: maximum threshold related to requests;
+ `--etcd-*`: access etcd certificate and etcd server address;
+ `--bind-address`: https monitoring IP cannot be `127.0.0.1`, otherwise the outside world cannot access its secure port 6443;
+ `--secret-port`: https listening port;
+ `--insecure-port=0`: Turn off listening to http insecure port (8080);
+ `--tls-*-file`: Specify the certificate, private key and CA file used by apiserver;
+ `--audit-*`: Configure audit policy and audit log file related parameters;
+ `--client-ca-file`: Verify the certificate carried by the client (kue-controller-manager, kube-scheduler, kubelet, kube-proxy, etc.) request;
+ `--enable-bootstrap-token-auth`: Enable token authentication for kubelet bootstrap;
+ `--requestheader-*`: configuration parameters related to the aggregator layer of kube-apiserver, required for proxy-client & HPA;
+ `--requestheader-client-ca-file`: used to sign the certificate specified by `--proxy-client-cert-file` and `--proxy-client-key-file`; used when metric aggregator is enabled ;
+ `--requestheader-allowed-names`: cannot be empty, the value is the CN name of the `--proxy-client-cert-file` certificate separated by commas, here set to "aggregator";
+ `--service-account-key-file`: the public key file for signing ServiceAccount Token, and kube-controller-manager's `--service-account-private-key-file` specifies the private key file, and the two are used in pairs;
+ `--runtime-config=api/all=true`: Enable all versions of APIs, such as autoscaling/v2alpha1;
+ `--authorization-mode=Node,RBAC`, `--anonymous-auth=false`: Enable Node and RBAC authorization modes and reject unauthorized requests;
+ `--enable-admission-plugins`: enable some plugins that are closed by default;
+ `--allow-privileged`: run containers with privileged permissions;
+ `--apiserver-count=3`: Specify the number of apiserver instances;
+ `--event-ttl`: Specify the storage time of events;
+ `--kubelet-*`: If specified, use https to access kubelet APIs; you need to define RBAC rules for the user corresponding to the certificate (the user of the kubernetes*.pem certificate above is kubernetes), otherwise it will prompt unauthorized access when accessing the kubelet API. ;
+ `--proxy-client-*`: the certificate used by apiserver to access metrics-server;
+ `--service-cluster-ip-range`: Specify the Service Cluster IP address range;
+ `--service-node-port-range`: Specify the port range of NodePort;

If the kube-apiserver machine **does not** run kube-proxy, you also need to add the `--enable-aggregator-routing=true` parameter;

Regarding `--requestheader-XXX` related parameters, please refer to:

+ https://github.com/kubernetes-incubator/apiserver-builder/blob/master/docs/concepts/auth.md
+ https://docs.bitnami.com/kubernetes/how-to/configure-autoscaling-custom-metrics/

Notice:
1. The CA certificate specified by `--requestheader-client-ca-file` must have `client auth and server auth`;
2. If `--requestheader-allowed-names` is not empty, and the CN name of the `--proxy-client-cert-file` certificate is not in allowed-names, subsequent viewing of the metrics of the node or pods will fail, with the following prompt:
  ``` bash
  $ kubectl top nodes
  Error from server (Forbidden): nodes.metrics.k8s.io is forbidden: User "aggregator" cannot list resource "nodes" in API group "metrics.k8s.io" at the cluster scope
  ```

## Create and distribute kube-apiserver systemd unit files to each node

Replace the variables in the template file to generate systemd unit files for each node:

``` bash
cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
for (( i=0; i < 3; i++ ))
  do
    sed -e "s/##NODE_NAME##/${NODE_NAMES[i]}/" -e "s/##NODE_IP##/${NODE_IPS[i]}/" kube-apiserver.service.template > kube-apiserver-${NODE_IPS[i]}.service 
  done
ls kube-apiserver*.service
```
```
kube-apiserver-103.172.238.94.service  kube-apiserver-103.172.239.71.service  kube-apiserver-103.172.239.9.service
```
+ NODE_NAMES and NODE_IPS are bash arrays of the same length, which are node names and corresponding IPs respectively;

Distribute the generated systemd unit file:

``` bash
cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo ">>> ${node_ip}"
    scp kube-apiserver-${node_ip}.service root@${node_ip}:/etc/systemd/system/kube-apiserver.service
  done
```

## Start the kube-apiserver service

``` bash
source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo ">>> ${node_ip}"
    ssh root@${node_ip} "mkdir -p ${K8S_DIR}/kube-apiserver"
    ssh root@${node_ip} "systemctl daemon-reload && systemctl enable kube-apiserver && systemctl restart kube-apiserver"
  done
```

## Check the running status of kube-apiserver

``` bash
source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo ">>> ${node_ip}"
    ssh root@${node_ip} "systemctl status kube-apiserver |grep 'Active:'"
  done
```

Make sure the status is `active (running)`, otherwise check the log to confirm the reason:

``` bash
journalctl -u kube-apiserver
```
```
[root@idc-k8s-01 work]# journalctl -u kube-apiserver
-- Logs begin at Sun 2024-03-31 12:02:38 +07, end at Mon 2024-04-01 11:51:12 +07. --
Apr 01 11:49:35 idc-k8s-01 systemd[1]: Starting Kubernetes API Server...
Apr 01 11:49:35 idc-k8s-01 kube-apiserver[13479]: Flag --insecure-port has been deprecated, This flag will be removed in a future version.
Apr 01 11:49:35 idc-k8s-01 kube-apiserver[13479]: I0401 11:49:35.175682   13479 flags.go:33] FLAG: --add-dir-header="false"
Apr 01 11:49:35 idc-k8s-01 kube-apiserver[13479]: I0401 11:49:35.175781   13479 flags.go:33] FLAG: --address="127.0.0.1"
Apr 01 11:49:35 idc-k8s-01 kube-apiserver[13479]: I0401 11:49:35.175799   13479 flags.go:33] FLAG: --admission-control="[]"
Apr 01 11:49:35 idc-k8s-01 kube-apiserver[13479]: I0401 11:49:35.175817   13479 flags.go:33] FLAG: --admission-control-config-file=""
Apr 01 11:49:35 idc-k8s-01 kube-apiserver[13479]: I0401 11:49:35.175832   13479 flags.go:33] FLAG: --advertise-address="103.172.239.9"
Apr 01 11:49:35 idc-k8s-01 kube-apiserver[13479]: I0401 11:49:35.175846   13479 flags.go:33] FLAG: --allow-privileged="true"
Apr 01 11:49:35 idc-k8s-01 kube-apiserver[13479]: I0401 11:49:35.175861   13479 flags.go:33] FLAG: --alsologtostderr="false"
Apr 01 11:49:35 idc-k8s-01 kube-apiserver[13479]: I0401 11:49:35.175873   13479 flags.go:33] FLAG: --anonymous-auth="false"
Apr 01 11:49:35 idc-k8s-01 kube-apiserver[13479]: I0401 11:49:35.175886   13479 flags.go:33] FLAG: --api-audiences="[]"
Apr 01 11:49:35 idc-k8s-01 kube-apiserver[13479]: I0401 11:49:35.175905   13479 flags.go:33] FLAG: --apiserver-count="3"
Apr 01 11:49:35 idc-k8s-01 kube-apiserver[13479]: I0401 11:49:35.175941   13479 flags.go:33] FLAG: --audit-dynamic-configuration="true"
Apr 01 11:49:35 idc-k8s-01 kube-apiserver[13479]: I0401 11:49:35.175954   13479 flags.go:33] FLAG: --audit-log-batch-buffer-size="10000"
Apr 01 11:49:35 idc-k8s-01 kube-apiserver[13479]: I0401 11:49:35.175970   13479 flags.go:33] FLAG: --audit-log-batch-max-size="1"
Apr 01 11:49:35 idc-k8s-01 kube-apiserver[13479]: I0401 11:49:35.175983   13479 flags.go:33] FLAG: --audit-log-batch-max-wait="0s"
Apr 01 11:49:35 idc-k8s-01 kube-apiserver[13479]: I0401 11:49:35.176011   13479 flags.go:33] FLAG: --audit-log-batch-throttle-burst="0"
Apr 01 11:49:35 idc-k8s-01 kube-apiserver[13479]: I0401 11:49:35.176023   13479 flags.go:33] FLAG: --audit-log-batch-throttle-enable="false
Apr 01 11:49:35 idc-k8s-01 kube-apiserver[13479]: I0401 11:49:35.176036   13479 flags.go:33] FLAG: --audit-log-batch-throttle-qps="0"
Apr 01 11:49:35 idc-k8s-01 kube-apiserver[13479]: I0401 11:49:35.176053   13479 flags.go:33] FLAG: --audit-log-format="json"
Apr 01 11:49:35 idc-k8s-01 kube-apiserver[13479]: I0401 11:49:35.176066   13479 flags.go:33] FLAG: --audit-log-maxage="15"
Apr 01 11:49:35 idc-k8s-01 kube-apiserver[13479]: I0401 11:49:35.176078   13479 flags.go:33] FLAG: --audit-log-maxbackup="3"
Apr 01 11:49:35 idc-k8s-01 kube-apiserver[13479]: I0401 11:49:35.176092   13479 flags.go:33] FLAG: --audit-log-maxsize="100"
Apr 01 11:49:35 idc-k8s-01 kube-apiserver[13479]: I0401 11:49:35.176104   13479 flags.go:33] FLAG: --audit-log-mode="blocking"
Apr 01 11:49:35 idc-k8s-01 kube-apiserver[13479]: I0401 11:49:35.176117   13479 flags.go:33] FLAG: --audit-log-path="/data/k8s/k8s/kube-api
Apr 01 11:49:35 idc-k8s-01 kube-apiserver[13479]: I0401 11:49:35.176130   13479 flags.go:33] FLAG: --audit-log-truncate-enabled="true"
Apr 01 11:49:35 idc-k8s-01 kube-apiserver[13479]: I0401 11:49:35.176142   13479 flags.go:33] FLAG: --audit-log-truncate-max-batch-size="104
Apr 01 11:49:35 idc-k8s-01 kube-apiserver[13479]: I0401 11:49:35.176157   13479 flags.go:33] FLAG: --audit-log-truncate-max-event-size="102
Apr 01 11:49:35 idc-k8s-01 kube-apiserver[13479]: I0401 11:49:35.176181   13479 flags.go:33] FLAG: --audit-log-version="audit.k8s.io/v1"
Apr 01 11:49:35 idc-k8s-01 kube-apiserver[13479]: I0401 11:49:35.176195   13479 flags.go:33] FLAG: --audit-policy-file="/etc/kubernetes/aud
Apr 01 11:49:35 idc-k8s-01 kube-apiserver[13479]: I0401 11:49:35.176208   13479 flags.go:33] FLAG: --audit-webhook-batch-buffer-size="10000
Apr 01 11:49:35 idc-k8s-01 kube-apiserver[13479]: I0401 11:49:35.176222   13479 flags.go:33] FLAG: --audit-webhook-batch-initial-backoff="1
Apr 01 11:49:35 idc-k8s-01 kube-apiserver[13479]: I0401 11:49:35.176234   13479 flags.go:33] FLAG: --audit-webhook-batch-max-size="400"
```

## Check cluster status

``` bash
$ kubectl cluster-info
Kubernetes master is running at https://103.172.239.9:6443

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.

$ kubectl get all --all-namespaces
NAMESPACE   NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
default     service/kubernetes   ClusterIP   10.254.0.1   <none>        443/TCP   3m53s

$ kubectl get componentstatuses
NAME                 AGE
controller-manager   <unknown>
scheduler            <unknown>
etcd-0               <unknown>
etcd-2               <unknown>
etcd-1               <unknown>
```
+ There are bugs in Kubernetes 1.16.6 that cause the returned result to always be `<unknown>`, but `kubectl get cs -o yaml` can return the correct result;
```yaml
[root@idc-k8s-01 work]# kubectl get cs -o yaml
apiVersion: v1
items:
- apiVersion: v1
  conditions:
  - message: 'Get http://127.0.0.1:10252/healthz: dial tcp 127.0.0.1:10252: connect:
      connection refused'
    status: "False"
    type: Healthy
  kind: ComponentStatus
  metadata:
    creationTimestamp: null
    name: controller-manager
    selfLink: /api/v1/componentstatuses/controller-manager
- apiVersion: v1
  conditions:
  - message: 'Get http://127.0.0.1:10251/healthz: dial tcp 127.0.0.1:10251: connect:
      connection refused'
    status: "False"
    type: Healthy
  kind: ComponentStatus
  metadata:
    creationTimestamp: null
    name: scheduler
    selfLink: /api/v1/componentstatuses/scheduler
- apiVersion: v1
  conditions:
  - message: '{"health":"true"}'
    status: "True"
    type: Healthy
  kind: ComponentStatus
  metadata:
    creationTimestamp: null
    name: etcd-1
    selfLink: /api/v1/componentstatuses/etcd-1
- apiVersion: v1
  conditions:
  - message: '{"health":"true"}'
    status: "True"
    type: Healthy
  kind: ComponentStatus
  metadata:
    creationTimestamp: null
    name: etcd-2
    selfLink: /api/v1/componentstatuses/etcd-2
- apiVersion: v1
  conditions:
  - message: '{"health":"true"}'
    status: "True"
    type: Healthy
  kind: ComponentStatus
  metadata:
    creationTimestamp: null
    name: etcd-0
    selfLink: /api/v1/componentstatuses/etcd-0
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""
```


## Check the port listened by kube-apiserver

``` bash
$ sudo netstat -lnpt|grep kube
tcp        0      0 103.172.239.9:6443     0.0.0.0:*               LISTEN      101442/kube-apiserv
```
+ 6443: Secure port for receiving https requests, authenticating and authorizing all requests;
+ Since the non-secure port is closed, there is no listening on 8080;