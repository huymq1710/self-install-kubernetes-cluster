tags: master, kube-scheduler

# 05-4. Deploy a highly available kube-scheduler cluster

<!-- TOC -->

- [05-4. Deploy high availability kube-scheduler cluster] (#05-4-Deploy high availability-kube-scheduler-cluster)
     - [Create kube-scheduler certificate and private key](#create-kube-scheduler-certificate and private key)
     - [Create and distribute kubeconfig files](#create and distribute-kubeconfig-files)
     - [Create kube-scheduler configuration file](#create-kube-scheduler-configuration file)
     - [Create kube-scheduler systemd unit template file](#create-kube-scheduler-systemd-unit-template file)
     - [Create and distribute kube-scheduler systemd unit files for each node](#Create and distribute-kube-scheduler-systemd-unit-file for each node)
     - [Start kube-scheduler service](#start-kube-scheduler-service)
     - [Check service running status](#check service running status)
     - [View the output metrics](#View the output-metrics)
     - [View current leader](#View current-leader)
     - [Test the high availability of the kube-scheduler cluster](#test-kube-scheduler-cluster's high availability)

<!-- /TOC -->

This document describes the steps to deploy a highly available kube-scheduler cluster.

The cluster contains 3 nodes. After startup, a leader node will be generated through a competitive election mechanism, and the other nodes will be in a blocking state. When the leader node becomes unavailable, the remaining nodes will elect a new leader node again to ensure service availability.

To ensure communication security, this document first generates an `x509` certificate and private key. kube-scheduler uses this certificate in the following two situations:

1. Communicate with the secure port of kube-apiserver;
2. Output metrics in prometheus format on the **secure port** (https, 10251);

Note: Unless otherwise specified, all operations in this document are performed on the idc-k8s-01 node.

## Create kube-scheduler certificate and private key

Create a certificate signing request:

``` bash
cd /opt/k8s/work
cat > kube-scheduler-csr.json <<EOF
{
    "CN": "system:kube-scheduler",
    "hosts": [
      "127.0.0.1",
      "103.172.239.9",
      "103.172.238.94",
      "103.172.239.71"
    ],
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
      {
        "C": "CN",
        "ST": "BeiJing",
        "L": "BeiJing",
        "O": "system:kube-scheduler",
        "OU": "opsnull"
      }
    ]
}
EOF
```
+ The hosts list contains **all** kube-scheduler node IPs;
+ CN and O are both `system:kube-scheduler`, and the built-in ClusterRoleBindings `system:kube-scheduler` of kubernetes will give kube-scheduler the necessary permissions to work;

Generate certificate and private key:

``` bash
cd /opt/k8s/work
cfssl gencert -ca=/opt/k8s/work/ca.pem \
  -ca-key=/opt/k8s/work/ca-key.pem \
  -config=/opt/k8s/work/ca-config.json \
  -profile=kubernetes kube-scheduler-csr.json | cfssljson -bare kube-scheduler
ls kube-scheduler*pem
```
```
kube-scheduler-key.pem  kube-scheduler.pem
```
Distribute the generated certificate and private key to all master nodes:

``` bash
cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo ">>> ${node_ip}"
    scp kube-scheduler*.pem root@${node_ip}:/etc/kubernetes/cert/
  done
```

## Create and distribute kubeconfig files

kube-scheduler accesses the apiserver using a kubeconfig file, which provides the apiserver address, embedded CA certificate, and kube-scheduler certificate:

``` bash
cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
kubectl config set-cluster kubernetes \
  --certificate-authority=/opt/k8s/work/ca.pem \
  --embed-certs=true \
  --server="https://##NODE_IP##:6443" \
  --kubeconfig=kube-scheduler.kubeconfig

kubectl config set-credentials system:kube-scheduler \
  --client-certificate=kube-scheduler.pem \
  --client-key=kube-scheduler-key.pem \
  --embed-certs=true \
  --kubeconfig=kube-scheduler.kubeconfig

kubectl config set-context system:kube-scheduler \
  --cluster=kubernetes \
  --user=system:kube-scheduler \
  --kubeconfig=kube-scheduler.kubeconfig

kubectl config use-context system:kube-scheduler --kubeconfig=kube-scheduler.kubeconfig
```

Distribute kubeconfig to all master nodes:

``` bash
cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo ">>> ${node_ip}"
    sed -e "s/##NODE_IP##/${node_ip}/" kube-scheduler.kubeconfig > kube-scheduler-${node_ip}.kubeconfig
    scp kube-scheduler-${node_ip}.kubeconfig root@${node_ip}:/etc/kubernetes/kube-scheduler.kubeconfig
  done
```

```
>>> 103.172.239.9
kube-scheduler-103.172.239.9.kubeconfig                                                                  100% 6391     3.9MB/s   00:00
>>> 103.172.238.94
kube-scheduler-103.172.238.94.kubeconfig                                                                 100% 6392     3.5MB/s   00:00
>>> 103.172.239.71
kube-scheduler-103.172.239.71.kubeconfig                                                                 100% 6392     3.2MB/s   00:00
```

## Create kube-scheduler configuration file

``` bash
cd /opt/k8s/work
cat >kube-scheduler.yaml.template <<EOF
apiVersion: kubescheduler.config.k8s.io/v1alpha1
kind: KubeSchedulerConfiguration
bindTimeoutSeconds: 600
clientConnection:
  burst: 200
  kubeconfig: "/etc/kubernetes/kube-scheduler.kubeconfig"
  qps: 100
enableContentionProfiling: false
enableProfiling: true
hardPodAffinitySymmetricWeight: 1
healthzBindAddress: ##NODE_IP##:10251
leaderElection:
  leaderElect: true
metricsBindAddress: ##NODE_IP##:10251
EOF
```
+ `--kubeconfig`: Specify the kubeconfig file path, which kube-scheduler uses to connect and verify kube-apiserver;
+ `--leader-elect=true`: Cluster operating mode, enabling the election function; the node selected as leader is responsible for processing work, and other nodes are in a blocking state;

Replace variables in template files:

``` bash
cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
for (( i=0; i < 3; i++ ))
  do
    sed -e "s/##NODE_NAME##/${NODE_NAMES[i]}/" -e "s/##NODE_IP##/${NODE_IPS[i]}/" kube-scheduler.yaml.template > kube-scheduler-${NODE_IPS[i]}.yaml
  done
ls kube-scheduler*.yaml
```
```
kube-scheduler-103.172.238.94.yaml  kube-scheduler-103.172.239.71.yaml  kube-scheduler-103.172.239.9.yaml
```
+ NODE_NAMES and NODE_IPS are bash arrays of the same length, which are node names and corresponding IPs respectively;

Distribute the kube-scheduler configuration file to all master nodes:

``` bash
cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo ">>> ${node_ip}"
    scp kube-scheduler-${node_ip}.yaml root@${node_ip}:/etc/kubernetes/kube-scheduler.yaml
  done
```
+ Renamed to kube-scheduler.yaml;

## Create kube-scheduler systemd unit template file

``` bash
cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
cat > kube-scheduler.service.template <<EOF
[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

[Service]
WorkingDirectory=${K8S_DIR}/kube-scheduler
ExecStart=/opt/k8s/bin/kube-scheduler \\
  --config=/etc/kubernetes/kube-scheduler.yaml \\
  --bind-address=##NODE_IP## \\
  --secure-port=10259 \\
  --port=0 \\
  --tls-cert-file=/etc/kubernetes/cert/kube-scheduler.pem \\
  --tls-private-key-file=/etc/kubernetes/cert/kube-scheduler-key.pem \\
  --authentication-kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \\
  --client-ca-file=/etc/kubernetes/cert/ca.pem \\
  --requestheader-allowed-names="" \\
  --requestheader-client-ca-file=/etc/kubernetes/cert/ca.pem \\
  --requestheader-extra-headers-prefix="X-Remote-Extra-" \\
  --requestheader-group-headers=X-Remote-Group \\
  --requestheader-username-headers=X-Remote-User \\
  --authorization-kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \\
  --logtostderr=true \\
  --v=2
Restart=always
RestartSec=5
StartLimitInterval=0

[Install]
WantedBy=multi-user.target
EOF
```

## Create and distribute kube-scheduler systemd unit files for each node

Replace the variables in the template file and create systemd unit files for each node:

``` bash
cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
for (( i=0; i < 3; i++ ))
  do
    sed -e "s/##NODE_NAME##/${NODE_NAMES[i]}/" -e "s/##NODE_IP##/${NODE_IPS[i]}/" kube-scheduler.service.template > kube-scheduler-${NODE_IPS[i]}.service 
  done
ls kube-scheduler*.service
```
```
kube-scheduler-103.172.238.94.service  kube-scheduler-103.172.239.71.service  kube-scheduler-103.172.239.9.service
```
Distribute systemd unit files to all master nodes:

``` bash
cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo ">>> ${node_ip}"
    scp kube-scheduler-${node_ip}.service root@${node_ip}:/etc/systemd/system/kube-scheduler.service
  done
```

## Start the kube-scheduler service

``` bash
source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo ">>> ${node_ip}"
    ssh root@${node_ip} "mkdir -p ${K8S_DIR}/kube-scheduler"
    ssh root@${node_ip} "systemctl daemon-reload && systemctl enable kube-scheduler && systemctl restart kube-scheduler"
  done
```

## Check service running status

``` bash
source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo ">>> ${node_ip}"
    ssh root@${node_ip} "systemctl status kube-scheduler|grep Active"
  done
```

Make sure the status is `active (running)`, otherwise check the log to confirm the reason:

``` bash
[root@idc-k8s-01 work]# journalctl -u kube-scheduler
-- Logs begin at Sun 2024-03-31 12:02:38 +07, end at Mon 2024-04-01 13:20:55 +07. --
Apr 01 13:20:36 idc-k8s-01 systemd[1]: Started Kubernetes Scheduler.
Apr 01 13:20:36 idc-k8s-01 kube-scheduler[14600]: I0401 13:20:36.815485   14600 flags.go:33] FLAG: --add-dir-header="false"
Apr 01 13:20:36 idc-k8s-01 kube-scheduler[14600]: I0401 13:20:36.815610   14600 flags.go:33] FLAG: --address="0.0.0.0"
Apr 01 13:20:36 idc-k8s-01 kube-scheduler[14600]: I0401 13:20:36.815631   14600 flags.go:33] FLAG: --algorithm-provider=""
Apr 01 13:20:36 idc-k8s-01 kube-scheduler[14600]: I0401 13:20:36.815645   14600 flags.go:33] FLAG: --alsologtostderr="false"
Apr 01 13:20:36 idc-k8s-01 kube-scheduler[14600]: I0401 13:20:36.815658   14600 flags.go:33] FLAG: --authentication-kubeconfig="/etc/kubern
Apr 01 13:20:36 idc-k8s-01 kube-scheduler[14600]: I0401 13:20:36.815674   14600 flags.go:33] FLAG: --authentication-skip-lookup="false"
Apr 01 13:20:36 idc-k8s-01 kube-scheduler[14600]: I0401 13:20:36.815690   14600 flags.go:33] FLAG: --authentication-token-webhook-cache-ttl
Apr 01 13:20:36 idc-k8s-01 kube-scheduler[14600]: I0401 13:20:36.815719   14600 flags.go:33] FLAG: --authentication-tolerate-lookup-failure
Apr 01 13:20:36 idc-k8s-01 kube-scheduler[14600]: I0401 13:20:36.815734   14600 flags.go:33] FLAG: --authorization-always-allow-paths="[/he
Apr 01 13:20:36 idc-k8s-01 kube-scheduler[14600]: I0401 13:20:36.815762   14600 flags.go:33] FLAG: --authorization-kubeconfig="/etc/kuberne
Apr 01 13:20:36 idc-k8s-01 kube-scheduler[14600]: I0401 13:20:36.815776   14600 flags.go:33] FLAG: --authorization-webhook-cache-authorized
Apr 01 13:20:36 idc-k8s-01 kube-scheduler[14600]: I0401 13:20:36.815790   14600 flags.go:33] FLAG: --authorization-webhook-cache-unauthoriz
Apr 01 13:20:36 idc-k8s-01 kube-scheduler[14600]: I0401 13:20:36.815804   14600 flags.go:33] FLAG: --bind-address="103.172.239.9"
Apr 01 13:20:36 idc-k8s-01 kube-scheduler[14600]: I0401 13:20:36.815821   14600 flags.go:33] FLAG: --cert-dir=""
Apr 01 13:20:36 idc-k8s-01 kube-scheduler[14600]: I0401 13:20:36.815834   14600 flags.go:33] FLAG: --client-ca-file="/etc/kubernetes/cert/c
Apr 01 13:20:36 idc-k8s-01 kube-scheduler[14600]: I0401 13:20:36.815848   14600 flags.go:33] FLAG: --config="/etc/kubernetes/kube-scheduler
Apr 01 13:20:36 idc-k8s-01 kube-scheduler[14600]: I0401 13:20:36.815862   14600 flags.go:33] FLAG: --contention-profiling="false"
Apr 01 13:20:36 idc-k8s-01 kube-scheduler[14600]: I0401 13:20:36.815875   14600 flags.go:33] FLAG: --feature-gates=""
Apr 01 13:20:36 idc-k8s-01 kube-scheduler[14600]: I0401 13:20:36.815893   14600 flags.go:33] FLAG: --hard-pod-affinity-symmetric-weight="1"
Apr 01 13:20:36 idc-k8s-01 kube-scheduler[14600]: I0401 13:20:36.815910   14600 flags.go:33] FLAG: --help="false"
Apr 01 13:20:36 idc-k8s-01 kube-scheduler[14600]: I0401 13:20:36.815942   14600 flags.go:33] FLAG: --http2-max-streams-per-connection="0"
Apr 01 13:20:36 idc-k8s-01 kube-scheduler[14600]: I0401 13:20:36.815970   14600 flags.go:33] FLAG: --kube-api-burst="100"
Apr 01 13:20:36 idc-k8s-01 kube-scheduler[14600]: I0401 13:20:36.815985   14600 flags.go:33] FLAG: --kube-api-content-type="application/vnd
Apr 01 13:20:36 idc-k8s-01 kube-scheduler[14600]: I0401 13:20:36.815999   14600 flags.go:33] FLAG: --kube-api-qps="50"
Apr 01 13:20:36 idc-k8s-01 kube-scheduler[14600]: I0401 13:20:36.816017   14600 flags.go:33] FLAG: --kubeconfig=""
Apr 01 13:20:36 idc-k8s-01 kube-scheduler[14600]: I0401 13:20:36.816030   14600 flags.go:33] FLAG: --leader-elect="true"
Apr 01 13:20:36 idc-k8s-01 kube-scheduler[14600]: I0401 13:20:36.816044   14600 flags.go:33] FLAG: --leader-elect-lease-duration="15s"
Apr 01 13:20:36 idc-k8s-01 kube-scheduler[14600]: I0401 13:20:36.816058   14600 flags.go:33] FLAG: --leader-elect-renew-deadline="10s"
Apr 01 13:20:36 idc-k8s-01 kube-scheduler[14600]: I0401 13:20:36.816071   14600 flags.go:33] FLAG: --leader-elect-resource-lock="endpoints"
Apr 01 13:20:36 idc-k8s-01 kube-scheduler[14600]: I0401 13:20:36.816084   14600 flags.go:33] FLAG: --leader-elect-resource-name="kube-sched
Apr 01 13:20:36 idc-k8s-01 kube-scheduler[14600]: I0401 13:20:36.816099   14600 flags.go:33] FLAG: --leader-elect-resource-namespace="kube-
Apr 01 13:20:36 idc-k8s-01 kube-scheduler[14600]: I0401 13:20:36.816112   14600 flags.go:33] FLAG: --leader-elect-retry-period="2s"

```

## View the output metrics

Note: The following commands are executed on the kube-scheduler node.

kube-scheduler listens on ports 10251 and 10259:
+ 10251: Receive http requests, non-secure port, no authentication and authorization required;
+ 10259: Receive https request, secure port, authentication and authorization required;

Both interfaces provide external access to `/metrics` and `/healthz`.

```
$ sudo netstat -lnpt |grep kube-sch
tcp        0      0 103.172.239.9:10251     0.0.0.0:*               LISTEN      14600/kube-schedule
tcp        0      0 103.172.239.9:10259     0.0.0.0:*               LISTEN      14600/kube-schedule
```

``` bash
$ curl -s http://103.172.239.9:10251/metrics |head
# HELP apiserver_audit_event_total [ALPHA] Counter of audit events generated and sent to the audit backend.
# TYPE apiserver_audit_event_total counter
apiserver_audit_event_total 0
# HELP apiserver_audit_requests_rejected_total [ALPHA] Counter of apiserver requests rejected due to an error in audit logging backend.
# TYPE apiserver_audit_requests_rejected_total counter
apiserver_audit_requests_rejected_total 0
# HELP apiserver_client_certificate_expiration_seconds [ALPHA] Distribution of the remaining lifetime on the certificate used to authenticate a request.
# TYPE apiserver_client_certificate_expiration_seconds histogram
apiserver_client_certificate_expiration_seconds_bucket{le="0"} 0
apiserver_client_certificate_expiration_seconds_bucket{le="1800"} 0
```

``` bash
$ curl -s --cacert /opt/k8s/work/ca.pem --cert /opt/k8s/work/admin.pem --key /opt/k8s/work/admin-key.pem https://103.172.239.9:10259/metrics |head
# HELP apiserver_audit_event_total [ALPHA] Counter of audit events generated and sent to the audit backend.
# TYPE apiserver_audit_event_total counter
apiserver_audit_event_total 0
# HELP apiserver_audit_requests_rejected_total [ALPHA] Counter of apiserver requests rejected due to an error in audit logging backend.
# TYPE apiserver_audit_requests_rejected_total counter
apiserver_audit_requests_rejected_total 0
# HELP apiserver_client_certificate_expiration_seconds [ALPHA] Distribution of the remaining lifetime on the certificate used to authenticate a request.
# TYPE apiserver_client_certificate_expiration_seconds histogram
apiserver_client_certificate_expiration_seconds_bucket{le="0"} 0
apiserver_client_certificate_expiration_seconds_bucket{le="1800"} 0
```

## View the current leader

``` bash
$ kubectl get endpoints kube-scheduler --namespace=kube-system  -o yaml
apiVersion: v1
kind: Endpoints
metadata:
  annotations:
    control-plane.alpha.kubernetes.io/leader: '{"holderIdentity":"idc-k8s-01_09de6013-ef37-4ce8-a12c-1534d3a3e5d5","leaseDurationSeconds":15,"acquireTime":"2024-04-01T06:20:37Z","renewTime":"2024-04-01T06:23:54Z","leaderTransitions":0}'
  creationTimestamp: "2024-04-01T06:20:37Z"
  name: kube-scheduler
  namespace: kube-system
  resourceVersion: "2798"
  selfLink: /api/v1/namespaces/kube-system/endpoints/kube-scheduler
  uid: 1a743d73-1ea2-4710-bc0e-a568778e2959
```

It can be seen that the current leader is the idc-k8s-01 node.

## Test the high availability of kube-scheduler cluster

Just find one or two master nodes, stop the kube-scheduler service, and see if other nodes have obtained leader permissions.
