tags: master, kube-controller-manager

# 05-3. Deploy a highly available kube-controller-manager cluster

<!-- TOC -->

- 05-3. Deploy high availability kube-controller-manager cluster
     - [Create kube-controller-manager certificate and private key](#create-kube-controller-manager-certificate and private key)
     - [Create and distribute kubeconfig files](#create and distribute-kubeconfig-files)
     - [Create kube-controller-manager systemd unit template file](#create-kube-controller-manager-systemd-unit-template file)
     - [Create and distribute kube-controller-mananger systemd unit files for each node](#Create and distribute-kube-controller-mananger-systemd-unit-file for each node)
     - [Start kube-controller-manager service](#start-kube-controller-manager-service)
     - [Check service running status](#check service running status)
     - [View the output metrics](#View the output-metrics)
     - [View current leader](#View current-leader)
     - [Test the high availability of the kube-controller-manager cluster](#test-kube-controller-manager-the high availability of the cluster)
     - [reference](#reference)

<!-- /TOC -->

This document describes the steps to deploy a highly available kube-controller-manager cluster.

The cluster contains 3 nodes. After startup, a leader node will be generated through a competitive election mechanism, and the other nodes will be in a blocking state. When the leader node is unavailable, the blocked node will elect a new leader node again to ensure service availability.

To ensure communication security, this document first generates an `x509` certificate and private key. kube-controller-manager uses this certificate in the following two situations:

1. Communicate with the secure port of kube-apiserver;
2. Output metrics in prometheus format on the **secure port** (https, 10252);

Note: Unless otherwise specified, all operations in this document are performed on the idc-k8s-01 node.

## Create kube-controller-manager certificate and private key

Create a certificate signing request:

``` bash
cd /opt/k8s/work
cat > kube-controller-manager-csr.json <<EOF
{
    "CN": "system:kube-controller-manager",
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "hosts": [
      "127.0.0.1",
      "103.172.239.9",
      "103.172.238.94",
      "103.172.239.71"
    ],
    "names": [
      {
        "C": "CN",
        "ST": "BeiJing",
        "L": "BeiJing",
        "O": "system:kube-controller-manager",
        "OU": "opsnull"
      }
    ]
}
EOF
```
+ The hosts list contains **all** kube-controller-manager node IPs;
+ CN and O are both `system:kube-controller-manager`, and kubernetes' built-in ClusterRoleBindings `system:kube-controller-manager` gives kube-controller-manager the necessary permissions to work.

Generate certificate and private key:

``` bash
cd /opt/k8s/work
cfssl gencert -ca=/opt/k8s/work/ca.pem \
  -ca-key=/opt/k8s/work/ca-key.pem \
  -config=/opt/k8s/work/ca-config.json \
  -profile=kubernetes kube-controller-manager-csr.json | cfssljson -bare kube-controller-manager
ls kube-controller-manager*pem
```
```
[root@idc-k8s-01 work]# ls kube-controller-manager*pem
kube-controller-manager-key.pem  kube-controller-manager.pem
```

Distribute the generated certificate and private key to all master nodes:

``` bash
cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo ">>> ${node_ip}"
    scp kube-controller-manager*.pem root@${node_ip}:/etc/kubernetes/cert/
  done
```

## Create and distribute kubeconfig files

kube-controller-manager uses the kubeconfig file to access the apiserver, which provides information such as the apiserver address, embedded CA certificate, and kube-controller-manager certificate:

``` bash
cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
kubectl config set-cluster kubernetes \
  --certificate-authority=/opt/k8s/work/ca.pem \
  --embed-certs=true \
  --server="https://##NODE_IP##:6443" \
  --kubeconfig=kube-controller-manager.kubeconfig

kubectl config set-credentials system:kube-controller-manager \
  --client-certificate=kube-controller-manager.pem \
  --client-key=kube-controller-manager-key.pem \
  --embed-certs=true \
  --kubeconfig=kube-controller-manager.kubeconfig

kubectl config set-context system:kube-controller-manager \
  --cluster=kubernetes \
  --user=system:kube-controller-manager \
  --kubeconfig=kube-controller-manager.kubeconfig

kubectl config use-context system:kube-controller-manager --kubeconfig=kube-controller-manager.kubeconfig
```
+ kube-controller-manager and kube-apiserver are mixed, so you can directly access kube-apiserver through **node IP**;

Distribute kubeconfig to all master nodes:

``` bash
cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo ">>> ${node_ip}"
    sed -e "s/##NODE_IP##/${node_ip}/" kube-controller-manager.kubeconfig > kube-controller-manager-${node_ip}.kubeconfig
    scp kube-controller-manager-${node_ip}.kubeconfig root@${node_ip}:/etc/kubernetes/kube-controller-manager.kubeconfig
  done
```
```
>>> 103.172.239.9
kube-controller-manager-103.172.239.9.kubeconfig                                                         100% 6463     3.1MB/s   00:00
>>> 103.172.238.94
kube-controller-manager-103.172.238.94.kubeconfig                                                        100% 6464     2.5MB/s   00:00
>>> 103.172.239.71
kube-controller-manager-103.172.239.71.kubeconfig                                                        100% 6464     1.9MB/s   00:00
```

## Create kube-controller-manager systemd unit template file

``` bash
cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
cat > kube-controller-manager.service.template <<EOF
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

[Service]
WorkingDirectory=${K8S_DIR}/kube-controller-manager
ExecStart=/opt/k8s/bin/kube-controller-manager \\
  --profiling \\
  --cluster-name=kubernetes \\
  --controllers=*,bootstrapsigner,tokencleaner \\
  --kube-api-qps=1000 \\
  --kube-api-burst=2000 \\
  --leader-elect \\
  --use-service-account-credentials\\
  --concurrent-service-syncs=2 \\
  --bind-address=##NODE_IP## \\
  --secure-port=10252 \\
  --tls-cert-file=/etc/kubernetes/cert/kube-controller-manager.pem \\
  --tls-private-key-file=/etc/kubernetes/cert/kube-controller-manager-key.pem \\
  --port=0 \\
  --authentication-kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \\
  --client-ca-file=/etc/kubernetes/cert/ca.pem \\
  --requestheader-allowed-names="aggregator" \\
  --requestheader-client-ca-file=/etc/kubernetes/cert/ca.pem \\
  --requestheader-extra-headers-prefix="X-Remote-Extra-" \\
  --requestheader-group-headers=X-Remote-Group \\
  --requestheader-username-headers=X-Remote-User \\
  --authorization-kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \\
  --cluster-signing-cert-file=/etc/kubernetes/cert/ca.pem \\
  --cluster-signing-key-file=/etc/kubernetes/cert/ca-key.pem \\
  --experimental-cluster-signing-duration=876000h \\
  --horizontal-pod-autoscaler-sync-period=10s \\
  --concurrent-deployment-syncs=10 \\
  --concurrent-gc-syncs=30 \\
  --node-cidr-mask-size=24 \\
  --service-cluster-ip-range=${SERVICE_CIDR} \\
  --pod-eviction-timeout=6m \\
  --terminated-pod-gc-threshold=10000 \\
  --root-ca-file=/etc/kubernetes/cert/ca.pem \\
  --service-account-private-key-file=/etc/kubernetes/cert/ca-key.pem \\
  --kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \\
  --logtostderr=true \\
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
```
+ `--port=0`: Turn off listening to non-secure ports (http), while the `--address` parameter is invalid and the `--bind-address` parameter is valid;
+ `--secure-port=10252`, `--bind-address=0.0.0.0`: listen to https /metrics requests on port 10252 on all network interfaces;
+ `--kubeconfig`: Specify the kubeconfig file path, which kube-controller-manager uses to connect and authenticate kube-apiserver;
+ `--authentication-kubeconfig` and `--authorization-kubeconfig`: kube-controller-manager uses them to connect to apiserver and authenticate and authorize client requests. `kube-controller-manager` no longer uses `--tls-ca-file` to verify the Client certificate requesting https metrics. If these two kubeconfig parameters are not configured, the client's request to connect to the kube-controller-manager https port will be rejected (prompt for insufficient permissions).
+ `--cluster-signing-*-file`: Sign the certificate created by TLS Bootstrap;
+ `--experimental-cluster-signing-duration`: Specify the validity period of the TLS Bootstrap certificate;
+ `--root-ca-file`: The CA certificate placed in the container ServiceAccount is used to verify the certificate of kube-apiserver;
+ `--service-account-private-key-file`: The private key file for signing the Token in ServiceAccount must be paired with the public key file specified by `--service-account-key-file` of kube-apiserver;
+ `--service-cluster-ip-range`: Specify the Service Cluster IP network segment, which must be consistent with the parameter of the same name in kube-apiserver;
+ `--leader-elect=true`: Cluster operating mode, enabling the election function; the node selected as leader is responsible for processing work, and other nodes are in a blocking state;
+ `--controllers=*,bootstrapsigner,tokencleaner`: list of enabled controllers, tokencleaner is used to automatically clean expired Bootstrap tokens;
+ `--horizontal-pod-autoscaler-*`: custom metrics related parameters, supports autoscaling/v2alpha1;
+ `--tls-cert-file`, `--tls-private-key-file`: Server certificate and secret key used when outputting metrics using https;
+ `--use-service-account-credentials=true`: Each controller in kube-controller-manager uses serviceaccount to access kube-apiserver;

## Create and distribute kube-controller-mananger systemd unit files to each node

Replace the variables in the template file and create systemd unit files for each node:

``` bash
cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
for (( i=0; i < 3; i++ ))
  do
    sed -e "s/##NODE_NAME##/${NODE_NAMES[i]}/" -e "s/##NODE_IP##/${NODE_IPS[i]}/" kube-controller-manager.service.template > kube-controller-manager-${NODE_IPS[i]}.service 
  done
ls kube-controller-manager*.service
```
```
kube-controller-manager-103.172.238.94.service  kube-controller-manager-103.172.239.9.service
kube-controller-manager-103.172.239.71.service
```
Distribute to all master nodes:

``` bash
cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo ">>> ${node_ip}"
    scp kube-controller-manager-${node_ip}.service root@${node_ip}:/etc/systemd/system/kube-controller-manager.service
  done
```

## Start the kube-controller-manager service

``` bash
source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo ">>> ${node_ip}"
    ssh root@${node_ip} "mkdir -p ${K8S_DIR}/kube-controller-manager"
    ssh root@${node_ip} "systemctl daemon-reload && systemctl enable kube-controller-manager && systemctl restart kube-controller-manager"
  done
```
```
>>> 103.172.239.9
Created symlink from /etc/systemd/system/multi-user.target.wants/kube-controller-manager.service to /etc/systemd/system/kube-controller-manager.service.
>>> 103.172.238.94
Created symlink from /etc/systemd/system/multi-user.target.wants/kube-controller-manager.service to /etc/systemd/system/kube-controller-manager.service.
>>> 103.172.239.71
Created symlink from /etc/systemd/system/multi-user.target.wants/kube-controller-manager.service to /etc/systemd/system/kube-controller-manager.service.
```
## Check service running status

``` bash
source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo ">>> ${node_ip}"
    ssh root@${node_ip} "systemctl status kube-controller-manager|grep Active"
  done
```

Make sure the status is `active (running)`, otherwise check the log to confirm the reason:
```
>>> 103.172.239.9
   Active: active (running) since Mon 2024-04-01 12:59:12 +07; 24s ago
>>> 103.172.238.94
   Active: active (running) since Mon 2024-04-01 12:59:13 +07; 24s ago
>>> 103.172.239.71
   Active: active (running) since Mon 2024-04-01 12:59:14 +07; 23s ago
```
``` bash
journalctl -u kube-controller-manager

-- Logs begin at Sun 2024-03-31 12:02:38 +07, end at Mon 2024-04-01 12:59:44 +07. --
Apr 01 12:59:12 idc-k8s-01 systemd[1]: Started Kubernetes Controller Manager.
Apr 01 12:59:12 idc-k8s-01 kube-controller-manager[14098]: Flag --port has been deprecated, see --secure-port instead.
Apr 01 12:59:12 idc-k8s-01 kube-controller-manager[14098]: I0401 12:59:12.448717   14098 flags.go:33] FLAG: --add-dir-header="false"
Apr 01 12:59:12 idc-k8s-01 kube-controller-manager[14098]: I0401 12:59:12.448778   14098 flags.go:33] FLAG: --address="0.0.0.0"
Apr 01 12:59:12 idc-k8s-01 kube-controller-manager[14098]: I0401 12:59:12.448790   14098 flags.go:33] FLAG: --allocate-node-cidrs="false"
Apr 01 12:59:12 idc-k8s-01 kube-controller-manager[14098]: I0401 12:59:12.448805   14098 flags.go:33] FLAG: --allow-untagged-cloud="false"
Apr 01 12:59:12 idc-k8s-01 kube-controller-manager[14098]: I0401 12:59:12.448817   14098 flags.go:33] FLAG: --alsologtostderr="false"
Apr 01 12:59:12 idc-k8s-01 kube-controller-manager[14098]: I0401 12:59:12.448828   14098 flags.go:33] FLAG: --attach-detach-reconcile-sync-
Apr 01 12:59:12 idc-k8s-01 kube-controller-manager[14098]: I0401 12:59:12.448845   14098 flags.go:33] FLAG: --authentication-kubeconfig="/e
Apr 01 12:59:12 idc-k8s-01 kube-controller-manager[14098]: I0401 12:59:12.448862   14098 flags.go:33] FLAG: --authentication-skip-lookup="f
Apr 01 12:59:12 idc-k8s-01 kube-controller-manager[14098]: I0401 12:59:12.448876   14098 flags.go:33] FLAG: --authentication-token-webhook-
Apr 01 12:59:12 idc-k8s-01 kube-controller-manager[14098]: I0401 12:59:12.448889   14098 flags.go:33] FLAG: --authentication-tolerate-looku
Apr 01 12:59:12 idc-k8s-01 kube-controller-manager[14098]: I0401 12:59:12.448896   14098 flags.go:33] FLAG: --authorization-always-allow-pa
Apr 01 12:59:12 idc-k8s-01 kube-controller-manager[14098]: I0401 12:59:12.448910   14098 flags.go:33] FLAG: --authorization-kubeconfig="/et
Apr 01 12:59:12 idc-k8s-01 kube-controller-manager[14098]: I0401 12:59:12.448918   14098 flags.go:33] FLAG: --authorization-webhook-cache-a
Apr 01 12:59:12 idc-k8s-01 kube-controller-manager[14098]: I0401 12:59:12.448927   14098 flags.go:33] FLAG: --authorization-webhook-cache-u
Apr 01 12:59:12 idc-k8s-01 kube-controller-manager[14098]: I0401 12:59:12.448935   14098 flags.go:33] FLAG: --bind-address="103.172.239.9"
Apr 01 12:59:12 idc-k8s-01 kube-controller-manager[14098]: I0401 12:59:12.448944   14098 flags.go:33] FLAG: --cert-dir=""
Apr 01 12:59:12 idc-k8s-01 kube-controller-manager[14098]: I0401 12:59:12.448951   14098 flags.go:33] FLAG: --cidr-allocator-type="RangeAll
Apr 01 12:59:12 idc-k8s-01 kube-controller-manager[14098]: I0401 12:59:12.448959   14098 flags.go:33] FLAG: --client-ca-file="/etc/kubernet
Apr 01 12:59:12 idc-k8s-01 kube-controller-manager[14098]: I0401 12:59:12.448966   14098 flags.go:33] FLAG: --cloud-config=""
Apr 01 12:59:12 idc-k8s-01 kube-controller-manager[14098]: I0401 12:59:12.448973   14098 flags.go:33] FLAG: --cloud-provider=""
Apr 01 12:59:12 idc-k8s-01 kube-controller-manager[14098]: I0401 12:59:12.448980   14098 flags.go:33] FLAG: --cloud-provider-gce-lb-src-cid
Apr 01 12:59:12 idc-k8s-01 kube-controller-manager[14098]: I0401 12:59:12.448992   14098 flags.go:33] FLAG: --cluster-cidr=""
Apr 01 12:59:12 idc-k8s-01 kube-controller-manager[14098]: I0401 12:59:12.449000   14098 flags.go:33] FLAG: --cluster-name="kubernetes"
Apr 01 12:59:12 idc-k8s-01 kube-controller-manager[14098]: I0401 12:59:12.449008   14098 flags.go:33] FLAG: --cluster-signing-cert-file="/e
Apr 01 12:59:12 idc-k8s-01 kube-controller-manager[14098]: I0401 12:59:12.449016   14098 flags.go:33] FLAG: --cluster-signing-key-file="/et
Apr 01 12:59:12 idc-k8s-01 kube-controller-manager[14098]: I0401 12:59:12.449023   14098 flags.go:33] FLAG: --concurrent-deployment-syncs="
Apr 01 12:59:12 idc-k8s-01 kube-controller-manager[14098]: I0401 12:59:12.449036   14098 flags.go:33] FLAG: --concurrent-endpoint-syncs="5"
Apr 01 12:59:12 idc-k8s-01 kube-controller-manager[14098]: I0401 12:59:12.449044   14098 flags.go:33] FLAG: --concurrent-gc-syncs="30"
Apr 01 12:59:12 idc-k8s-01 kube-controller-manager[14098]: I0401 12:59:12.449052   14098 flags.go:33] FLAG: --concurrent-namespace-syncs="1
Apr 01 12:59:12 idc-k8s-01 kube-controller-manager[14098]: I0401 12:59:12.449059   14098 flags.go:33] FLAG: --concurrent-replicaset-syncs="
Apr 01 12:59:12 idc-k8s-01 kube-controller-manager[14098]: I0401 12:59:12.449067   14098 flags.go:33] FLAG: --concurrent-resource-quota-syn
```

kube-controller-manager listens to port 10252 and receives https requests:

``` bash
$ sudo netstat -lnpt | grep kube-cont
tcp        0      0 103.172.239.9:10252     0.0.0.0:*               LISTEN      14098/kube-controll
```

## View the output metrics

Note: The following commands are executed on the kube-controller-manager node.

``` bash
$ curl -s --cacert /opt/k8s/work/ca.pem --cert /opt/k8s/work/admin.pem --key /opt/k8s/work/admin-key.pem https://103.172.239.9:10252/metrics |head
# HELP apiserver_audit_event_total [ALPHA] Counter of audit events generated and sent to the audit backend.
# TYPE apiserver_audit_event_total counter
apiserver_audit_event_total 0
# HELP apiserver_audit_requests_rejected_total [ALPHA] Counter of apiserver requests rejected due to an error in audit logging backend.
# TYPE apiserver_audit_requests_rejected_total counter
apiserver_audit_requests_rejected_total 0
# HELP apiserver_client_certificate_expiration_seconds [ALPHA] Distribution of the remaining lifetime on the certificate used to authenticate a request.
# TYPE apiserver_client_certificate_expiration_seconds histogram
apiserver_client_certificate_expiration_seconds_bucket{le="0"} 0
apiserver_client_certificate_expiration_seconds_bucket{le="1800"} 0
```

## View the current leader

``` bash
$ kubectl get endpoints kube-controller-manager --namespace=kube-system  -o yaml
apiVersion: v1
kind: Endpoints
metadata:
  annotations:
    control-plane.alpha.kubernetes.io/leader: '{"holderIdentity":"idc-k8s-01_adbda074-c882-46f1-8267-f2e7eee4c941","leaseDurationSeconds":15,"acquireTime":"2024-04-01T05:59:12Z","renewTime":"2024-04-01T06:03:37Z","leaderTransitions":0}'
  creationTimestamp: "2024-04-01T05:59:12Z"
  name: kube-controller-manager
  namespace: kube-system
  resourceVersion: "1733"
  selfLink: /api/v1/namespaces/kube-system/endpoints/kube-controller-manager
  uid: 6eb96e22-790b-40ea-a796-74f145e2f3b0
```

It can be seen that the current leader is the idc-k8s-01 node.

## Test the high availability of the kube-controller-manager cluster

Stop the kube-controller-manager service on one or two nodes, and observe the logs of other nodes to see if leader permissions have been obtained.
```
[root@idc-k8s-01 work]# systemctl stop kube-controller-manager

# Current leader is the idc-k8s-02
[root@idc-k8s-01 work]# kubectl get endpoints kube-controller-manager --namespace=kube-system  -o yaml
apiVersion: v1
kind: Endpoints
metadata:
  annotations:
    control-plane.alpha.kubernetes.io/leader: '{"holderIdentity":"idc-k8s-02_1cf03fd6-fc2c-4519-9ce9-71e3c0cd0214","leaseDurationSeconds":15,"acquireTime":"2024-04-01T06:05:57Z","renewTime":"2024-04-01T06:06:55Z","leaderTransitions":1}'
  creationTimestamp: "2024-04-01T05:59:12Z"
  name: kube-controller-manager
  namespace: kube-system
  resourceVersion: "1884"
  selfLink: /api/v1/namespaces/kube-system/endpoints/kube-controller-manager
  uid: 6eb96e22-790b-40ea-a796-74f145e2f3b0

[root@idc-k8s-01 work]# systemctl start kube-controller-manager
```
## Refer to

1. About controller permissions and use-service-account-credentials parameters: https://github.com/kubernetes/kubernetes/issues/48208
2. `kubelet` authentication and authorization: https://kubernetes.io/docs/admin/kubelet-authentication-authorization/#kubelet-authorization